# My-AI
Personal AI Assistant - Local-First Intelligence Platform

## Overview
A privacy-focused personal AI assistant that runs locally first, with optional cloud fallbacks. Built around Retrieval Augmented Generation (RAG) with your personal documents and integrated tools for productivity.

## Architecture
- **Local-First**: Primary processing using local models (Ollama/vLLM)
- **Cloud Fallback**: Optional external APIs (GPT/Claude) when needed
- **Tool-Based**: Extensible system with read-only integrations
- **Privacy-Centric**: Your data stays local by default

## Core Features
### Phase 0 (MVP)
- ğŸ“š **RAG Answer**: Search and cite from your personal document corpus
- ğŸ” **Drive Search**: Find files across connected storage
- ğŸŒ **Web Search**: Fetch public references with attribution

### Phase 1 (Planned)
- ğŸ“… **Calendar Integration**: Read events and scheduling data
- ğŸµ **Spotify Lookup**: Access playlists and music data

### Phase 2 (Future)
- ğŸ’° **Banking Read-Only**: Financial insights (optional, secure)

## API Endpoints
- `POST /v1/query` - Main query interface
- `POST /v1/ingest` - Add documents to knowledge base
- `POST /v1/chat` - Direct LLM gateway (local models)
- `POST /v1/embeddings` - Vectorize text for RAG

## Quick Start

### Prerequisites
- Python 3.11+
- [Ollama](https://ollama.ai/) installed and running
- Poetry (recommended) or pip

### 1. Install Ollama
Download and install Ollama from [ollama.ai](https://ollama.ai/)

### 2. Pull Required Models

#### For Smaller PCs (Laptops/No Dedicated GPU)
```bash
# Default model
ollama pull qwen3:1.7b

# Fallback model
ollama pull llama3.2:1b

# Coding assistance
ollama pull qwen2.5-coder:1.5b-instruct

# Reasoning model
ollama pull deepseek-r1:1.5b
```

#### For Computers with Dedicated GPUs
```bash
# Primary model
ollama pull qwen3:8b

# Fallback model
ollama pull llama3.1:8b

# Coding assistance
ollama pull qwen2.5-coder:7b

# Reasoning model
ollama pull deepseek-r1:7b
```

### 3. Clone and Setup
```bash
git clone <your-repo-url>
cd MY-AI
poetry install  # or pip install -r requirements.txt
```

### 4. Run the Application
```bash
# Development mode
make dev

# Or with Docker
make docker-dev
```

The API will be available at `http://localhost:8000`

## Security & Privacy
- âœ… Local processing by default
- âœ… No secrets in logs
- âœ… Parameterized queries only
- âœ… User data stays on your machine
- âœ… Optional cloud features require explicit consent

## Development Roadmap

### âœ… Step 0: Planning & Contracts (Complete)
- Software contracts and guardrails defined
- API surfaces and tool allowlists established
- Tier-2 routing strategy documented

### âœ… Step 1: API Scaffolding (Complete)
- Repository structure: Single-package format with `app/`, `llm/`, `rag/`, `agents/`, `connectors/`, `core/`
- Basic `/v1/query` endpoint with placeholder responses
- Health check endpoint
- LLM Gateway with Ollama integration

### âœ… Step 2: Local LLM Gateway (Complete)
- Ollama integration for local models
- `/v1/chat` and `/v1/embeddings` endpoints
- Configurable local/cloud model switching

### ğŸ“‹ Step 3: RAG MVP (Planned)
- Document ingestion and chunking pipeline
- SQLite + FAISS/sqlite-vec storage
- Cited answer retrieval from personal corpus

### ğŸ“‹ Step 4: AI Router (Planned)
- Natural language to `{tool, args}` JSON routing
- Schema validation and clarification flows
- Bounded LLM call counts (1-2 per request)

### ğŸ“‹ Step 5: Tool Integration (Planned)
- Drive search, web search, calendar lookup
- Read-only tool implementations
- Tool execution transcripts and logging

### ğŸ“‹ Step 6: Memory System (Planned)
- Short-term session memory
- Local storage with purge capabilities
- Foundation for future long-term memory

### ğŸ“‹ Step 7: Dynamic Corpus (Future)
- AI-proposed document updates
- Human approval workflow
- Provenance tracking and diff management

## Project Structure
```
MY-AI/
â”œâ”€â”€ app/                    # FastAPI application
â”‚   â”œâ”€â”€ main.py            # Application entry point
â”‚   â”œâ”€â”€ routes/            # API endpoints
â”‚   â””â”€â”€ services/          # Business logic
â”œâ”€â”€ llm/                   # LLM functionality
â”‚   â”œâ”€â”€ gateway.py         # LLM gateway
â”‚   â”œâ”€â”€ local.py           # Ollama client
â”‚   â””â”€â”€ external.py        # External API adapters
â”œâ”€â”€ rag/                   # Retrieval augmented generation
â”œâ”€â”€ agents/                # AI routing and tool orchestration
â”œâ”€â”€ connectors/            # External service integrations
â”œâ”€â”€ core/                  # Shared utilities and schemas
â”œâ”€â”€ tests/                 # Test files
â”œâ”€â”€ pyproject.toml         # Poetry configuration
â”œâ”€â”€ Makefile              # Development commands
â””â”€â”€ docker-compose.yml    # Local development with Ollama
```

## Development Status
ğŸš§ **Step 0 Complete, Step 1 In Progress** - See `/Documentation/software contract/` for detailed specifications

## Contributing
This is a personal project following a structured development path. See documentation for implementation details and architecture decisions.
